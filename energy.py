# -*- coding: utf-8 -*-
"""Energy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CiINs2VuCXhDLjdqJczoQYe2LlfLYnbk
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings
import pylab as pl
from sklearn import linear_model
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn import metrics
# %matplotlib inline
warnings.filterwarnings('ignore')

#Read the dataset
df = pd.read_csv('/content/drive/MyDrive/data.csv')
df.head()

#correlations between data
df.corr()

# correlation graphs
import seaborn as sns

sns.pairplot(df)

class ordinary_least_squares(object):
    pass
    #to see the coefficients of a regression model
    def __init__(self):
      self.coefficients = []
    
    def fit(self, X, y):
      if len(X.shape) == 1:
        X = self._reshape_x(X)
      X = self._concatenate_ones(X)
      self.coefficients = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose().dot(y))

    def predict(self, entry):
      b0 = self.coefficients[0]
      other_betas = self.coefficients[1:]

      prediction = b0

      for xi, bi in zip(entry, other_betas):
        prediction += (bi * xi)
      return prediction

    #reshape x to anything 2 dimensional
    def _reshape_x (self, X):
      return X.reshape(-1, 1)
    #create a vector of ones with the same number of elements the feature matrix has
    def _concatenate_ones(self, X):
      ones = np.ones(shape=X.shape[0]).reshape(-1, 1)
      return np.concatenate((ones, X), 1)

# Taking away the y from the data
X = df.drop('energy', axis = 1).values
X = df[[ 'loudness', 'popularity', 'valence', 'year']].values
y = df['energy'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)

model = ordinary_least_squares()

model.fit(X_train, y_train)

model.coefficients

y_preds = []

for row in X_train: 
  y_preds.append(model.predict(row))

pd.DataFrame({
    'Actual': y_train,
    'Predicted' : np.ravel(y_preds)
})

y_preds_tests = []

for row in X_test: 
  y_preds_tests.append(model.predict(row))

pd.DataFrame({
    'Actual': y_test,
    'Predicted' : np.ravel(y_preds_tests)
})

print('Mean Squared Error: %.2f' % np.sqrt(metrics.mean_squared_error(y_test,y_preds_tests)))

X= df[[ 'loudness', 'popularity', 'valence', 'year']]
y = df[['energy']]
X.head()

#split dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)


regr = linear_model.LinearRegression()
#train model
regr.fit(X_train,y_train)
#predict with test 
y_=regr.predict(X_test)
#get the cross validation score 
acc = cross_val_score(regr, X, y)



print('Coefficients:', regr.coef_)
print('Variance score: %.2f' % regr.score(X_train,y_train)) 
print('Mean Cross Validation Score: %.2f' % np.mean(acc))
print('Mean Squared Error: %.2f' % np.sqrt(metrics.mean_squared_error(y_test,y_)))

"""Queries"""

# valence (Ranges from 0 to 1)
# popularity (Ranges from 0 to 100)
# loudness (Float typically ranging from -60 to 0)
# year (Ranges from 1921 to 2020)

print("Loudness")
loudness = float(input())

print("Popularity")
popularity = float(input())

print("Valence")
valence = float(input())

print("year")
year = float(input())

query_x = np.array([[loudness, popularity, valence, year]])

query_y_h = model.predict(query_x[0])
print(query_y_h)

# valence (Ranges from 0 to 1)
# popularity (Ranges from 0 to 100)
# loudness (Float typically ranging from -60 to 0)
# year (Ranges from 1921 to 2020)

print("Loudness")
loudness = float(input())

print("Popularity")
popularity = float(input())

print("Valence")
valence = float(input())

print("year")
year = float(input())

query_x = np.array([[loudness, popularity, valence, year]])

query_y_f = regr.predict(query_x)
print(query_y_f)